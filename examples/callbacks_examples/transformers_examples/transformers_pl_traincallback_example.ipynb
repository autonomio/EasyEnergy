{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from easyenergy.callbacks.pytorch_lightning import TrainCallback\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pytorch lightning datamodule from the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUEDataModule(LightningDataModule):\n",
    "\n",
    "    task_text_field_map = {\n",
    "        \"cola\": [\"sentence\"],\n",
    "        \"sst2\": [\"sentence\"],\n",
    "        \"mrpc\": [\"sentence1\", \"sentence2\"],\n",
    "        \"qqp\": [\"question1\", \"question2\"],\n",
    "        \"stsb\": [\"sentence1\", \"sentence2\"],\n",
    "        \"mnli\": [\"premise\", \"hypothesis\"],\n",
    "        \"qnli\": [\"question\", \"sentence\"],\n",
    "        \"rte\": [\"sentence1\", \"sentence2\"],\n",
    "        \"wnli\": [\"sentence1\", \"sentence2\"],\n",
    "        \"ax\": [\"premise\", \"hypothesis\"],\n",
    "    }\n",
    "\n",
    "    glue_task_num_labels = {\n",
    "        \"cola\": 2,\n",
    "        \"sst2\": 2,\n",
    "        \"mrpc\": 2,\n",
    "        \"qqp\": 2,\n",
    "        \"stsb\": 1,\n",
    "        \"mnli\": 3,\n",
    "        \"qnli\": 2,\n",
    "        \"rte\": 2,\n",
    "        \"wnli\": 2,\n",
    "        \"ax\": 3,\n",
    "    }\n",
    "\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"mrpc\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = self.task_text_field_map[task_name]\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset = datasets.load_dataset(\"glue\", self.task_name)\n",
    "\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(\n",
    "                self.convert_to_features,\n",
    "                batched=True,\n",
    "                remove_columns=[\"label\"],\n",
    "            )\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.load_dataset(\"glue\", self.task_name)\n",
    "        AutoTokenizer.from_pretr\n",
    "        def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d218a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = GLUEDataModule(\"distilbert-base-uncased\")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7750f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pytorch lightning model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4228a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUETransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        task_name: str,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.metric = datasets.load_metric(\n",
    "            \"glue\", self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        if self.hparams.num_labels > 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.hparams.task_name == \"mnli\":\n",
    "            for i, output in enumerate(outputs):\n",
    "                # matched or mismatched\n",
    "                split = self.hparams.eval_splits[i].split(\"_\")[-1]\n",
    "                preds = torch.cat([x[\"preds\"] for x in output]).detach().cpu().numpy()\n",
    "                labels = torch.cat([x[\"labels\"] for x in output]).detach().cpu().numpy()\n",
    "                loss = torch.stack([x[\"loss\"] for x in output]).mean()\n",
    "                self.log(f\"val_loss_{split}\", loss, prog_bar=True)\n",
    "                split_metrics = {\n",
    "                    f\"{k}_{split}\": v for k, v in self.metric.compute(predictions=preds, references=labels).items()\n",
    "                }\n",
    "                self.log_dict(split_metrics, prog_bar=True)\n",
    "            return loss\n",
    "\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42) #set seed to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26720b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = GLUEDataModule(model_name_or_path=\"albert-base-v2\", task_name=\"cola\")\n",
    "dm.setup(\"fit\")\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"albert-base-v2\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a128061",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = TrainCallback() #initialize EasyEnergy Callback\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    callbacks=[cb] #Adding easynergy callback to pytorch lightning trainer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=dm) #Run training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
